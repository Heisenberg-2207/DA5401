{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import unicodedata\n",
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of locales\n",
    "locales = [\n",
    "    \"af-ZA\", \"da-DK\", \"de-DE\", \"en-US\", \"es-ES\", \"fr-FR\", \"fi-FI\", \"hu-HU\", \"is-IS\", \"it-IT\",\n",
    "    \"jv-ID\", \"lv-LV\", \"ms-MY\", \"nb-NO\", \"nl-NL\", \"pl-PL\", \"pt-PT\", \"ro-RO\", \"ru-RU\", \"sl-SL\",\n",
    "    \"sv-SE\", \"sq-AL\", \"sw-KE\", \"tl-PH\", \"tr-TR\", \"vi-VN\", \"cy-GB\"\n",
    "]\n",
    "\n",
    "# Function to deaccent characters\n",
    "def deaccent(text):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Load the MASSIVE dataset from Huggingface\n",
    "dataset = load_dataset(\"qanastek/MASSIVE\")\n",
    "\n",
    "# Filter the dataset for the relevant locales and partitions\n",
    "def filter_dataset(dataset, locales, partition):\n",
    "    filtered_data = dataset.filter(lambda x: x['locale'] in locales and x['partition'] == partition)\n",
    "    texts = [' '.join(deaccent(token) for token in utt) for utt in filtered_data['tokens']]\n",
    "    labels = filtered_data['locale']\n",
    "    return texts, labels\n",
    "\n",
    "# Get the training, validation, and test data\n",
    "train_texts, train_labels = filter_dataset(dataset['train'], locales, 'train')\n",
    "val_texts, val_labels = filter_dataset(dataset['validation'], locales, 'validation')\n",
    "test_texts, test_labels = filter_dataset(dataset['test'], locales, 'test')\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_labels)\n",
    "val_labels = label_encoder.transform(val_labels)\n",
    "test_labels = label_encoder.transform(test_labels)\n",
    "\n",
    "# Create a pipeline with a CountVectorizer and MultinomialNB\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_texts, train_labels)\n",
    "\n",
    "# Fine-tune the model with validation data\n",
    "model.fit(val_texts, val_labels)\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, texts, labels, partition_name):\n",
    "    predictions = model.predict(texts)\n",
    "    report = classification_report(labels, predictions, target_names=label_encoder.classes_)\n",
    "    print(f\"Performance metrics for {partition_name} partition:\")\n",
    "    print(report)\n",
    "\n",
    "# Report performance metrics\n",
    "evaluate_model(model, train_texts, train_labels, \"training\")\n",
    "evaluate_model(model, val_texts, val_labels, \"validation\")\n",
    "evaluate_model(model, test_texts, test_labels, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of locales and their respective continents\n",
    "locale_to_continent = {\n",
    "    \"af-ZA\": \"Africa\", \"da-DK\": \"Europe\", \"de-DE\": \"Europe\", \"en-US\": \"North America\", \"es-ES\": \"Europe\",\n",
    "    \"fr-FR\": \"Europe\", \"fi-FI\": \"Europe\", \"hu-HU\": \"Europe\", \"is-IS\": \"Europe\", \"it-IT\": \"Europe\",\n",
    "    \"jv-ID\": \"Asia\", \"lv-LV\": \"Europe\", \"ms-MY\": \"Asia\", \"nb-NO\": \"Europe\", \"nl-NL\": \"Europe\",\n",
    "    \"pl-PL\": \"Europe\", \"pt-PT\": \"Europe\", \"ro-RO\": \"Europe\", \"ru-RU\": \"Europe\", \"sl-SL\": \"Europe\",\n",
    "    \"sv-SE\": \"Europe\", \"sq-AL\": \"Europe\", \"sw-KE\": \"Africa\", \"tl-PH\": \"Asia\", \"tr-TR\": \"Asia\",\n",
    "    \"vi-VN\": \"Asia\", \"cy-GB\": \"Europe\"\n",
    "}\n",
    "\n",
    "# Function to deaccent characters\n",
    "def deaccent(text):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Load the MASSIVE dataset from Huggingface\n",
    "dataset = load_dataset(\"qanastek/MASSIVE\")\n",
    "\n",
    "# Filter the dataset for the relevant locales and partitions\n",
    "def filter_dataset(dataset, locales, partition):\n",
    "    filtered_data = dataset.filter(lambda x: x['locale'] in locales and x['partition'] == partition)\n",
    "    texts = [' '.join(deaccent(token) for token in utt) for utt in filtered_data['tokens']]\n",
    "    labels = [locale_to_continent[x['locale']] for x in filtered_data]\n",
    "    return texts, labels\n",
    "\n",
    "# Get the training, validation, and test data\n",
    "train_texts, train_labels = filter_dataset(dataset['train'], locale_to_continent.keys(), 'train')\n",
    "val_texts, val_labels = filter_dataset(dataset['validation'], locale_to_continent.keys(), 'validation')\n",
    "test_texts, test_labels = filter_dataset(dataset['test'], locale_to_continent.keys(), 'test')\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_labels)\n",
    "val_labels = label_encoder.transform(val_labels)\n",
    "test_labels = label_encoder.transform(test_labels)\n",
    "\n",
    "# Vectorize the text data using TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=10000, min_df=5, stop_words='english')\n",
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "X_val = vectorizer.transform(val_texts)\n",
    "X_test = vectorizer.transform(test_texts)\n",
    "\n",
    "# Implement Regularized Discriminant Analysis (RDA)\n",
    "class RegularizedDiscriminantAnalysis(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        self.alpha = alpha\n",
    "        self.lda = LinearDiscriminantAnalysis()\n",
    "        self.qda = QuadraticDiscriminantAnalysis()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.lda.fit(X.toarray(), y)\n",
    "        self.qda.fit(X.toarray(), y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        lda_pred = self.lda.predict_proba(X.toarray())\n",
    "        qda_pred = self.qda.predict_proba(X.toarray())\n",
    "        combined_pred = self.alpha * lda_pred + (1 - self.alpha) * qda_pred\n",
    "        return np.argmax(combined_pred, axis=1)\n",
    "\n",
    "# Create and train the RDA model\n",
    "rda_model = RegularizedDiscriminantAnalysis(alpha=0.5)\n",
    "rda_model.fit(X_train, train_labels)\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, X, y, partition_name):\n",
    "    predictions = model.predict(X)\n",
    "    report = classification_report(y, predictions, target_names=label_encoder.classes_)\n",
    "    print(f\"Performance metrics for {partition_name} partition:\")\n",
    "    print(report)\n",
    "\n",
    "# Report performance metrics\n",
    "evaluate_model(rda_model, X_train, train_labels, \"training\")\n",
    "evaluate_model(rda_model, X_val, val_labels, \"validation\")\n",
    "evaluate_model(rda_model, X_test, test_labels, \"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
