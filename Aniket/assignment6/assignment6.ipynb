{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import unicodedata\n",
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of locales\n",
    "locales = [\n",
    "    \"af-ZA\", \"da-DK\", \"de-DE\", \"en-US\", \"es-ES\", \"fr-FR\", \"fi-FI\", \"hu-HU\", \"is-IS\", \"it-IT\",\n",
    "    \"jv-ID\", \"lv-LV\", \"ms-MY\", \"nb-NO\", \"nl-NL\", \"pl-PL\", \"pt-PT\", \"ro-RO\", \"ru-RU\", \"sl-SL\",\n",
    "    \"sv-SE\", \"sq-AL\", \"sw-KE\", \"tl-PH\", \"tr-TR\", \"vi-VN\", \"cy-GB\"\n",
    "]\n",
    "\n",
    "# Function to deaccent characters\n",
    "def deaccent(text):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Load the MASSIVE dataset from Huggingface\n",
    "dataset = load_dataset(\"qanastek/MASSIVE\")\n",
    "\n",
    "# Filter the dataset for the relevant locales and partitions\n",
    "def filter_dataset(dataset, locales, partition):\n",
    "    filtered_data = dataset.filter(lambda x: x['locale'] in locales and x['partition'] == partition)\n",
    "    texts = [' '.join(deaccent(token) for token in utt) for utt in filtered_data['tokens']]\n",
    "    labels = filtered_data['locale']\n",
    "    return texts, labels\n",
    "\n",
    "# Get the training, validation, and test data\n",
    "train_texts, train_labels = filter_dataset(dataset['train'], locales, 'train')\n",
    "val_texts, val_labels = filter_dataset(dataset['validation'], locales, 'dev')\n",
    "test_texts, test_labels = filter_dataset(dataset['test'], locales, 'test')\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_labels)\n",
    "val_labels = label_encoder.transform(val_labels)\n",
    "test_labels = label_encoder.transform(test_labels)\n",
    "\n",
    "# Create a pipeline with a CountVectorizer and MultinomialNB\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_texts, train_labels)\n",
    "\n",
    "# Fine-tune the model with validation data\n",
    "model.fit(val_texts, val_labels)\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, texts, labels, partition_name):\n",
    "    predictions = model.predict(texts)\n",
    "    report = classification_report(labels, predictions, target_names=label_encoder.classes_)\n",
    "    print(f\"Performance metrics for {partition_name} partition:\")\n",
    "    print(report)\n",
    "\n",
    "# Report performance metrics\n",
    "evaluate_model(model, train_texts, train_labels, \"training\")\n",
    "evaluate_model(model, val_texts, val_labels, \"validation\")\n",
    "evaluate_model(model, test_texts, test_labels, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of locales and their respective continents\n",
    "locale_to_continent = {\n",
    "    \"af-ZA\": \"Africa\", \"da-DK\": \"Europe\", \"de-DE\": \"Europe\", \"en-US\": \"North America\", \"es-ES\": \"Europe\",\n",
    "    \"fr-FR\": \"Europe\", \"fi-FI\": \"Europe\", \"hu-HU\": \"Europe\", \"is-IS\": \"Europe\", \"it-IT\": \"Europe\",\n",
    "    \"jv-ID\": \"Asia\", \"lv-LV\": \"Europe\", \"ms-MY\": \"Asia\", \"nb-NO\": \"Europe\", \"nl-NL\": \"Europe\",\n",
    "    \"pl-PL\": \"Europe\", \"pt-PT\": \"Europe\", \"ro-RO\": \"Europe\", \"ru-RU\": \"Europe\", \"sl-SL\": \"Europe\",\n",
    "    \"sv-SE\": \"Europe\", \"sq-AL\": \"Europe\", \"sw-KE\": \"Africa\", \"tl-PH\": \"Asia\", \"tr-TR\": \"Asia\",\n",
    "    \"vi-VN\": \"Asia\", \"cy-GB\": \"Europe\"\n",
    "}\n",
    "\n",
    "# Function to deaccent characters\n",
    "def deaccent(text):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Load the MASSIVE dataset from Huggingface\n",
    "dataset = load_dataset(\"qanastek/MASSIVE\")\n",
    "\n",
    "# Filter the dataset for the relevant locales and partitions\n",
    "def filter_dataset(dataset, locales, partition):\n",
    "    filtered_data = dataset.filter(lambda x: x['locale'] in locales and x['partition'] == partition)\n",
    "    texts = [' '.join(deaccent(token) for token in utt) for utt in filtered_data['tokens']]\n",
    "    labels = [locale_to_continent[x['locale']] for x in filtered_data]\n",
    "    return texts, labels\n",
    "\n",
    "# Get the training, validation, and test data\n",
    "train_texts, train_labels = filter_dataset(dataset['train'], locale_to_continent.keys(), 'train')\n",
    "val_texts, val_labels = filter_dataset(dataset['validation'], locale_to_continent.keys(), 'dev')\n",
    "test_texts, test_labels = filter_dataset(dataset['test'], locale_to_continent.keys(), 'test')\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_labels)\n",
    "val_labels = label_encoder.transform(val_labels)\n",
    "test_labels = label_encoder.transform(test_labels)\n",
    "\n",
    "# Vectorize the text data using TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1000, min_df=5, stop_words='english')\n",
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "X_val = vectorizer.transform(val_texts)\n",
    "X_test = vectorizer.transform(test_texts)\n",
    "\n",
    "# Implement Regularized Discriminant Analysis (RDA)\n",
    "class RegularizedDiscriminantAnalysis(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        self.alpha = alpha\n",
    "        self.lda = LinearDiscriminantAnalysis()\n",
    "        self.qda = QuadraticDiscriminantAnalysis()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.lda.fit(X.toarray(), y)\n",
    "        self.qda.fit(X.toarray(), y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        lda_pred = self.lda.predict_proba(X.toarray())\n",
    "        qda_pred = self.qda.predict_proba(X.toarray())\n",
    "        combined_pred = self.alpha * lda_pred + (1 - self.alpha) * qda_pred\n",
    "        return np.argmax(combined_pred, axis=1)\n",
    "\n",
    "# Create and train the RDA model\n",
    "rda_model = RegularizedDiscriminantAnalysis(alpha=0.5)\n",
    "rda_model.fit(X_train, train_labels)\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, X, y, partition_name):\n",
    "    predictions = model.predict(X)\n",
    "    report = classification_report(y, predictions, target_names=label_encoder.classes_)\n",
    "    print(f\"Performance metrics for {partition_name} partition:\")\n",
    "    print(report)\n",
    "\n",
    "# Report performance metrics\n",
    "evaluate_model(rda_model, X_train, train_labels, \"training\")\n",
    "evaluate_model(rda_model, X_val, val_labels, \"validation\")\n",
    "evaluate_model(rda_model, X_test, test_labels, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search to find the best alpha\n",
    "param_grid = {'alpha': np.linspace(0, 1, 11)}\n",
    "grid_search = GridSearchCV(RegularizedDiscriminantAnalysis(), param_grid, scoring=make_scorer(accuracy_score), cv=5)\n",
    "grid_search.fit(X_train, train_labels)\n",
    "\n",
    "# Get the best alpha\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "print(f\"Best alpha: {best_alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Implement Regularized Discriminant Analysis (RDA)\n",
    "# class RegularizedDiscriminantAnalysis(BaseEstimator, ClassifierMixin):\n",
    "#     def __init__(self, alpha=0.5):\n",
    "#         self.alpha = alpha\n",
    "#         self.lda = LinearDiscriminantAnalysis()\n",
    "#         self.qda = QuadraticDiscriminantAnalysis()\n",
    "    \n",
    "#     def fit(self, X, y):\n",
    "#         self.lda.fit(X, y)\n",
    "#         self.qda.fit(X, y)\n",
    "#         return self\n",
    "    \n",
    "#     def predict(self, X):\n",
    "#         lda_pred = self.lda.predict_proba(X)\n",
    "#         qda_pred = self.qda.predict_proba(X)\n",
    "#         combined_pred = self.alpha * lda_pred + (1 - self.alpha) * qda_pred\n",
    "#         return np.argmax(combined_pred, axis=1)\n",
    "\n",
    "# # Define the list of locales and their respective continents\n",
    "# locale_to_continent = {\n",
    "#     \"af-ZA\": \"Africa\", \"da-DK\": \"Europe\", \"de-DE\": \"Europe\", \"en-US\": \"North America\", \"es-ES\": \"Europe\",\n",
    "#     \"fr-FR\": \"Europe\", \"fi-FI\": \"Europe\", \"hu-HU\": \"Europe\", \"is-IS\": \"Europe\", \"it-IT\": \"Europe\",\n",
    "#     \"jv-ID\": \"Asia\", \"lv-LV\": \"Europe\", \"ms-MY\": \"Asia\", \"nb-NO\": \"Europe\", \"nl-NL\": \"Europe\",\n",
    "#     \"pl-PL\": \"Europe\", \"pt-PT\": \"Europe\", \"ro-RO\": \"Europe\", \"ru-RU\": \"Europe\", \"sl-SL\": \"Europe\",\n",
    "#     \"sv-SE\": \"Europe\", \"sq-AL\": \"Europe\", \"sw-KE\": \"Africa\", \"tl-PH\": \"Asia\", \"tr-TR\": \"Asia\",\n",
    "#     \"vi-VN\": \"Asia\", \"cy-GB\": \"Europe\"\n",
    "# }\n",
    "\n",
    "# # Function to deaccent characters\n",
    "# def deaccent(text):\n",
    "#     return ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# # Load the MASSIVE dataset from Huggingface\n",
    "# dataset = load_dataset(\"qanastek/MASSIVE\")\n",
    "\n",
    "# # Filter the dataset for the relevant locales and partitions\n",
    "# def filter_dataset(dataset, locales, partition):\n",
    "#     filtered_data = dataset.filter(lambda x: x['locale'] in locales and x['partition'] == partition)\n",
    "#     texts = [' '.join(deaccent(token) for token in utt) for utt in filtered_data['tokens']]\n",
    "#     labels = [locale_to_continent[x['locale']] for x in filtered_data]\n",
    "#     return texts, labels\n",
    "\n",
    "# # Get the training, validation, and test data\n",
    "# train_texts, train_labels = filter_dataset(dataset['train'], locale_to_continent.keys(), 'train')\n",
    "# val_texts, val_labels = filter_dataset(dataset['validation'], locale_to_continent.keys(), 'dev')\n",
    "# test_texts, test_labels = filter_dataset(dataset['test'], locale_to_continent.keys(), 'test')\n",
    "\n",
    "# # Encode the labels\n",
    "# label_encoder = LabelEncoder()\n",
    "# train_labels = label_encoder.fit_transform(train_labels)\n",
    "# val_labels = label_encoder.transform(val_labels)\n",
    "# test_labels = label_encoder.transform(test_labels)\n",
    "\n",
    "# # Define a pipeline with TfidfVectorizer, TruncatedSVD, and RegularizedDiscriminantAnalysis\n",
    "# pipeline = Pipeline([\n",
    "#     ('vectorizer', TfidfVectorizer(stop_words='english')),\n",
    "#     ('svd', TruncatedSVD()),\n",
    "#     ('classifier', RegularizedDiscriminantAnalysis(alpha=0.5))\n",
    "# ])\n",
    "\n",
    "# # Define parameter grid for GridSearchCV\n",
    "# param_grid = {\n",
    "#     'vectorizer__max_features': [500, 1000, 1500, 2000, 2500, 3000],\n",
    "#     'svd__n_components': [50, 100, 150, 200],\n",
    "#     'classifier__alpha': np.linspace(0, 1, 11)\n",
    "# }\n",
    "\n",
    "# # Perform grid search to find the best max_features, n_components, and alpha\n",
    "# grid_search = GridSearchCV(pipeline, param_grid, scoring=make_scorer(accuracy_score), cv=5)\n",
    "# grid_search.fit(train_texts, train_labels)\n",
    "\n",
    "# # Get the best parameters\n",
    "# best_params = grid_search.best_params_\n",
    "# print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# # Train the model with the best parameters\n",
    "# best_vectorizer = TfidfVectorizer(max_features=best_params['vectorizer__max_features'], stop_words='english')\n",
    "# X_train = best_vectorizer.fit_transform(train_texts)\n",
    "# X_val = best_vectorizer.transform(val_texts)\n",
    "# X_test = best_vectorizer.transform(test_texts)\n",
    "\n",
    "# best_svd = TruncatedSVD(n_components=best_params['svd__n_components'])\n",
    "# X_train_svd = best_svd.fit_transform(X_train)\n",
    "# X_val_svd = best_svd.transform(X_val)\n",
    "# X_test_svd = best_svd.transform(X_test)\n",
    "\n",
    "# best_alpha = best_params['classifier__alpha']\n",
    "# rda_model = RegularizedDiscriminantAnalysis(alpha=best_alpha)\n",
    "# rda_model.fit(X_train_svd, train_labels)\n",
    "\n",
    "# # Evaluate the model\n",
    "# def evaluate_model(model, X, y, partition_name):\n",
    "#     predictions = model.predict(X)\n",
    "#     report = classification_report(y, predictions, target_names=label_encoder.classes_)\n",
    "#     print(f\"Performance metrics for {partition_name} partition:\")\n",
    "#     print(report)\n",
    "\n",
    "# # Report performance metrics\n",
    "# evaluate_model(rda_model, X_train_svd, train_labels, \"training\")\n",
    "# evaluate_model(rda_model, X_val_svd, val_labels, \"validation\")\n",
    "# evaluate_model(rda_model, X_test_svd, test_labels, \"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
