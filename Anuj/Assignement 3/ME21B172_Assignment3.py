# -*- coding: utf-8 -*-
"""Assignment3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wDahizlQLLKZjLQrSHo26A0Ic8PxyTTd

# Assignment 3
###Anuj Jagannath Said
###Roll number : ME21B172
"""

import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
import math
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm

df = pd.read_csv('Assignment3.csv')
data = df.to_numpy()
print(data.shape)
N = len(data[:,0])
ratio = 0.8
X_train = data[:round(ratio*N),:5]
y_train = data[:round(ratio*N),5]
X_test = data[round(ratio*N):,:5]
y_test = data[round(ratio*N):,5]

# Training
reg = LinearRegression(fit_intercept=True).fit(X_train, y_train)
weights  = np.transpose(reg.coef_.reshape(1,-1))
# print(weights.shape)
# print(X_train.shape)
y_predicted_train = np.matmul(X_train,weights).reshape(-1) + reg.intercept_
mse = (np.matmul(y_train-y_predicted_train,np.transpose(y_train-y_predicted_train))/len(y_train))**.5
print("RMSE error while training using OLS:")
print(mse)
print("Values of weights while training using OLS:")
print(weights)

# Testing
y_predicted_test = np.matmul(X_test,weights).reshape(-1) + reg.intercept_
mse  = (np.matmul(y_test-y_predicted_test,np.transpose(y_test-y_predicted_test))/ len(y_test))**.5
print("RMSE error while testing using OLS:")
print(mse)
print(weights)
plt.plot(np.arange(8000,12800),np.arange(8000,12800),color='b')
plt.plot(y_test,y_predicted_test,'o',color='g')
plt.plot(y_train,y_predicted_train,'o',color='r')
plt.legend(['y=x','Testing dataset','Training dataset'])
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Performance of final model')
plt.show()
print("RMSE error while testing for model with transformed regressors:")
print(mse)

"""##Performing EDA"""

sns.color_palette("YlOrBr", as_cmap=True)
ax = sns.heatmap(df.corr(), annot=True,cmap="YlOrBr")

sns.set_theme(style="ticks", color_codes=True)
g = sns.pairplot(df)
plt.show()

"""Form the above plots it is clear that $x4$ feature is completely redundant (as $x1$ and $x4$ are highly correlated features so we won't gain anything new). Hence we need to transform $x4$ feature.<br>
Similarly, from the figure it seems that $x2$ and $x5$ are related, but linearly they aren't$;$ as a result we see the correlation coefficient near to 0 (hence we won't be able to say that $x5$ is redundant feature).
##Transforming the features.
"""

print(np.corrcoef(y_train,X_train[:,3]**1))
print(np.corrcoef(y_train,X_train[:,3]**2))

"""From the adove correlation results it turns out that $x4^2$ is more correalted with $y$ than $x4$.<br>
Hence using the model as:<br>
$y = c1*x1 + c2*x2 + c3*x3 + c4*x4^2 + c5*x5 + c6,$<br>
"""

X_tranformed = np.array([X_train[:,0],X_train[:,1],X_train[:,2],X_train[:,3]**2,X_train[:,4],np.ones(len(y_train))])
est = sm.OLS(y_train, np.transpose(X_tranformed))
est2 = est.fit()
print(est2.summary())
weights  = np.transpose(est2.params.reshape(1,-1))
weights.shape
y_predicted = np.matmul(np.transpose(X_tranformed),weights).reshape(-1)
mse  = (np.matmul(y_train-y_predicted,np.transpose(y_train-y_predicted))/len(y_train))**.5
print("RMSE error while training for model with transformed regressors:")
print(mse)
print(weights)

"""From the summary we obtained, p-values of the $x3$ and $const$ features are beyond the threshold limit (< 0.05). Hence removing the $const$ and $x3$ feature we get,"""

X_tranformed = np.array([X_train[:,0],X_train[:,1],X_train[:,3]**2,X_train[:,4]])
est = sm.OLS(y_train, np.transpose(X_tranformed))
est2 = est.fit()
print(est2.summary())
weights  = np.transpose(est2.params.reshape(1,-1))
weights.shape
y_predicted = np.matmul(np.transpose(X_tranformed),weights).reshape(-1)
mse  = (np.matmul(y_train-y_predicted,np.transpose(y_train-y_predicted))/len(y_train))**.5
print("RMSE error while training for model with transformed regressors:")
print(mse)
print(weights)

"""P-values for all features are within the threshold along with the Root mean-squared error as 0.6446. Also, the condition number has dropped by the factor of 100 indicating the slight increase in the numerical stability.<br>
Hence using the model as:<br>
$y = c1*x1 + c2*x2 + c4*x4^2 + c5*x5,$<br>
"""

# Testing
X_tranformed_test = np.array([X_test[:,0],X_test[:,1],X_test[:,3]**2,X_test[:,4]])
y_predicted_test = np.matmul(np.transpose(X_tranformed_test),weights).reshape(-1)
mse  = (np.matmul(y_test-y_predicted_test,np.transpose(y_test-y_predicted_test))/ len(y_test))**.5
plt.plot(np.arange(8000,12800),np.arange(8000,12800),color='b')
plt.plot(y_test,y_predicted_test,'o',color='g')
plt.plot(y_train,y_predicted,'o',color='r')
plt.legend(['y=x','Testing dataset','Training dataset'])
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Performance of final model')
plt.show()
print("RMSE error while testing for model with transformed regressors:")
print(mse)

"""Comparision of initial model and the final model,

1.   Using Linear regression (without feature transformation)


>RMSE for initial model (training) = 20.821376861817605

>RMSE for initial model (testing) = 52.36447387683089

2.   Using Linear regression (with feature transformation)

>RMSE for initial model (training) = 0.6445964513579052

>RMSE for initial model (testing) = 0.8257895662283041

"""

import lazypredict
from lazypredict.Supervised import LazyRegressor
regressor = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)
models, predictions = regressor.fit(X_train, X_test, y_train, y_test)
print(models)
